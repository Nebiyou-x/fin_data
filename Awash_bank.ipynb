{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f19872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fe0eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 30 FAQs\n",
      "Sample FAQ:\n",
      "{\n",
      "  \"instruction\": \"Answer the following banking question\",\n",
      "  \"input\": \"What are loyalty points and how can I convert loyalty point to airtime charge?\",\n",
      "  \"output\": \"• Loyalty points are points awarded to customers per transaction using Awashbirr App or USSD. These points will be converted into airtime charge when you reach a minimum of 20 points.\\n• Loyalty points are points awarded to customers per transaction using Awashbirr App or USSD. These points will be converted into airtime charge when you reach a minimum of 20 points.\",\n",
      "  \"source\": \"https://www.awashbank.com/faq\",\n",
      "  \"category\": \"Awash Bank FAQ\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def scrape_awash_faqs(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "       \n",
    "        accordion = soup.find('div', class_='eael-adv-accordion')\n",
    "        \n",
    "        if not accordion:\n",
    "            print(\"Could not find FAQ accordion container\")\n",
    "            return []\n",
    "            \n",
    "        faq_list = []\n",
    "        \n",
    "        \n",
    "        faq_items = accordion.find_all('div', class_='eael-accordion-list')\n",
    "        \n",
    "        for item in faq_items:\n",
    "            try:\n",
    "                \n",
    "                question_div = item.find('span', class_='eael-accordion-tab-title')\n",
    "                question = question_div.get_text(strip=True) if question_div else \"\"\n",
    "                \n",
    "                answer_div = item.find('div', class_='eael-accordion-content')\n",
    "                answer = \"\"\n",
    "                \n",
    "                if answer_div:\n",
    "                    \n",
    "                    for element in answer_div.find_all(['p', 'ul', 'li']):\n",
    "                        if element.name == 'p':\n",
    "                            answer += element.get_text(strip=True) + \"\\n\"\n",
    "                        elif element.name == 'ul':\n",
    "                            for li in element.find_all('li'):\n",
    "                                answer += f\"• {li.get_text(strip=True)}\\n\"\n",
    "                        elif element.name == 'li':\n",
    "                            answer += f\"• {element.get_text(strip=True)}\\n\"\n",
    "                \n",
    "                if question and answer:\n",
    "                    faq_list.append({\n",
    "                        \"instruction\": \"Answer the following banking question\",\n",
    "                        \"input\": question,\n",
    "                        \"output\": answer.strip(),\n",
    "                        \"source\": url,\n",
    "                        \"category\": \"Awash Bank FAQ\"\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return faq_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Scraping failed: {str(e)}\")\n",
    "        return []\n",
    "url = \"https://www.awashbank.com/faq\"\n",
    "faqs = scrape_awash_faqs(url)\n",
    "\n",
    "\n",
    "if faqs:\n",
    "    with open('awash_bank_faqs.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(faqs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Successfully scraped {len(faqs)} FAQs\")\n",
    "    print(\"Sample FAQ:\")\n",
    "    print(json.dumps(faqs[0], indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"No FAQs found. Please check the URL or website structure.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
